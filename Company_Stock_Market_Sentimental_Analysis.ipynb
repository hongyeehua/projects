{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Company Stock Market Sentimental Analysis**\n",
        "\n",
        "**Hong Yee (Isaac) Hua**\n",
        "\n",
        "---\n",
        "\n",
        "Tired of sifting through analyst articles to gauge market sentiment and market news on a particular company?\n",
        "\n",
        "**Look no further.**\n",
        "\n",
        "This tool in Python is designed to obtain the most recent view of the market sentiment on a company, with a summarisation of recent market news.\n",
        "\n",
        "\n",
        "\n",
        "**NOTE:** You will need a Cohere API key and an Alpha Vantage API key (obtainable through account creation). Maybe if you, dear reader, can get me a job, then you wouldn't need to provide your own API key.. however, I am but a lowly student with a trial key. Can't exactly process hundreds of articles.\n",
        "\n",
        "---\n",
        "### **How does this work?**\n",
        "\n",
        "Well, I'm glad you asked since I've spent an embarrassing amount of time on this..\n",
        "\n",
        "Essentially,\n",
        "\n",
        "1. Alpha Vantage AI pulls and processes analyst articles on the ticker, through websites such as Bezinga, Zacks etc.\n",
        "2. Each article is processed through Newspaper to be read, and overall market sentiment is averaged out across all the articles through Alpha Vantage's natural language processing model.\n",
        "3. Taking samples of each article, we summarise using Cohere AI's summarisation model. We feed these articles through layers and summarise further, depending on the complexity of articles.\n",
        "4. Our end result is an average sentiment score across all the recent articles (number specified by user) and a final summarised paragraph of the most recent market news on the specified company.\n",
        "\n",
        "Enjoy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Improvements to be made:**\n",
        "- Include foreign companies which are not in US stock market, use a translation model to process articles in foreign languages\n",
        "- Splitting into cases $\\rightarrow$ trial users (more hallucinations for news summarisation) & professional users (more accurate and reliable for news summarisation)\n",
        "- Training model further to more efficiently\n",
        "- Raising appropriate errors when given invalid inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "fhjzyk2t3vSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOQIySJxVQAV",
        "outputId": "875bd477-1208-4d2f-91c0-f221d24766ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: alpha_vantage in /usr/local/lib/python3.9/dist-packages (2.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from alpha_vantage) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from alpha_vantage) (3.8.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->alpha_vantage) (22.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->alpha_vantage) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->alpha_vantage) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->alpha_vantage) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.9/dist-packages (0.2.8)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (6.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (2.27.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (6.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (8.4.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.9/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.9/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.9/dist-packages (4.0.4)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from cohere) (2.27.1)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from cohere) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.8.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.0->cohere) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.0->cohere) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "# install all libraries\n",
        "!pip install alpha_vantage\n",
        "!pip install requests\n",
        "!pip install newspaper3k\n",
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08L7trBuTkj1",
        "outputId": "b9c09840-98e7-4401-fdaa-818beb151913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import nltk\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "from newspaper import Article\n",
        "import random\n",
        "import cohere\n",
        "from newspaper.images import urllib\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ZItHaEOTTl",
        "outputId": "d526b7ca-e019-49ff-a0f7-4b2347704750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your Cohere API key?\n",
            "TqNim7S803C7Pz7pLmGYb38PZkiDeceA1wkFDDTf \n",
            "What is your AlphaVantage API key?\n",
            "KK6JVIIT1YZ51460\n",
            "What is your prospective company's ticker symbol?\n",
            "GOOG\n",
            "How many articles do you want to analyse? (Up to 200 for trial users)\n",
            "200\n",
            "\n",
            " Overall recent market sentiment:  \n",
            " Neutral \n",
            " Summarization of current market news:  \n",
            " Alphabet Inc. has announced that Google will invest $1 billion in Elon Musk's SpaceX. Google's investment will help SpaceX develop a satellite-based internet service that is operating in remote areas around the world and will also allow SpaceX to continue its ambitious goal of sending humans to Mars. The investment may also bring Google closer to fulfilling its own goal of expanding its cloud business. Google could use SpaceX's satellite network to offer cloud services to a wider range of customers. While SpaceX has not released any details about the deal, Google's investment comes at a time when the company is trying to catch up with Amazon Web Services and Microsoft Azure in the cloud business.\n"
          ]
        }
      ],
      "source": [
        "def get_yahoo_shortname(symbol): #function to lookup company ticker's name\n",
        "    response = urllib.request.urlopen(f'https://query2.finance.yahoo.com/v1/finance/search?q={symbol}')\n",
        "    content = response.read()\n",
        "    company = json.loads(content.decode('utf8'))['quotes'][0]['shortname']\n",
        "    return company\n",
        "\n",
        "def sentimental_analysis(): #big boy function\n",
        "    \"\"\"\n",
        "    Returns sentimental analysis on company ticker, using Cohere and Alpha Vantage.\n",
        "    \"\"\"\n",
        "\n",
        "    cohere_apikey = str(input(\"What is your Cohere API key?\\n\"))\n",
        "    alpha_vantage = str(input(\"What is your AlphaVantage API key?\\n\"))\n",
        "    ticker = str(input(\"What is your prospective company's ticker symbol?\\n\"))\n",
        "    limit = str(input(\"How many articles do you want to analyse? (Up to 200 for trial users)\\n\"))\n",
        "\n",
        "    co = cohere.Client(cohere_apikey)\n",
        "\n",
        "    url = ('https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=' \n",
        "           + f\"{ticker}\"+ f'&limit={limit}'+ '&apikey=' + f\"{alpha_vantage}\")\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "\n",
        "    article_urls = [] #initialising article url list and total ticker sentiment score\n",
        "    total_ticker_sentiment = 0.0\n",
        "\n",
        "    for a in range(0, len(data['feed'])): #Parses through each article\n",
        "        article_urls.append(data['feed'][a]['url']) #Appends url to the list\n",
        "        for t in range(0, len(data['feed'][a]['ticker_sentiment'])): #Parses through ticker list for particular ticker\n",
        "            if data['feed'][a]['ticker_sentiment'][t]['ticker'] == ticker: #Only grab the info about relevant ticker\n",
        "              total_ticker_sentiment += float(data['feed'][a]['ticker_sentiment'][t]['ticker_sentiment_score'])\n",
        "\n",
        "    average_ticker_sentiment = total_ticker_sentiment/len(data['feed'])\n",
        "\n",
        "    if average_ticker_sentiment <= -0.35:\n",
        "      sentiment = \"Bearish\"\n",
        "    elif average_ticker_sentiment <= -0.15:\n",
        "      sentiment = \"Slightly Bearish\"\n",
        "    elif average_ticker_sentiment < 0.15:\n",
        "      sentiment = \"Neutral\"\n",
        "    elif average_ticker_sentiment < 0.35:\n",
        "      sentiment = \"Slightly Bullish\"\n",
        "    else:\n",
        "      sentiment = \"Bullish\" \n",
        "\n",
        "    article_contents = []\n",
        "    company = get_yahoo_shortname(ticker)\n",
        "\n",
        "    for url in article_urls:\n",
        "      try: #Web scraping is not always legal and allowed, so we will do it for websites that allow it.\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article.nlp()\n",
        "        article_contents.append(article.text)\n",
        "      except:\n",
        "        continue #we skip summarizing the articles that do not allow for scraping\n",
        "      \n",
        "# Cutting all this out since I only have trial API key. (Use if premium plan)\n",
        "\n",
        "#    random_list_first = random.sample(range(0,len(article_contents)), \n",
        "#                                      k=(len(article_contents)//5)) #RNG for summarizing\n",
        "   \n",
        "#    summarized_articles = []\n",
        "\n",
        "#    for i in random_list_first: \n",
        "#      response = co.summarize(text=article_contents[i], model='summarize-xlarge', \n",
        "#                                length='long', format=\"paragraph\", temperature=0.1,\n",
        "#                                additional_command=f\"Summarise with the focus on the company: {company}\")\n",
        "#      summarized_articles.append(response.summary)\n",
        "    \n",
        "#    random_list_second = random.sample(range(0,len(summarized_articles)),\n",
        "#                                       k=len((summarized_articles)//2)) #second RNG\n",
        "#    second_random_articles = []\n",
        "\n",
        "#    for i in random_list_second: \n",
        "#      response = co.summarize(text=summarized_articles[i], model='summarize-xlarge', \n",
        "#                                length='long', format=\"paragraph\", temperature=0.1,\n",
        "#                                additional_command=f\"Summarise with the focus on the company: {company}\")\n",
        "#      second_random_articles.append(response.summary)\n",
        "\n",
        "#    random_list_final = random.sample(range(0,len(second_random_articles)),\n",
        "#                                      k=1)\n",
        "    \n",
        "#    for i in random_list_final:\n",
        "#        response = co.summarize(text=second_random_articles[i], model='summarize-xlarge', \n",
        "#                                length='long', format=\"paragraph\", temperature=0.1,\n",
        "#                                additional_command=f\"Summarise with the focus on the company: {company}\")\n",
        "    \n",
        "    random_list_first = random.sample(range(0,len(article_contents)), \n",
        "                                      k=min(3,len(article_contents))) #RNG for summarizing\n",
        "   \n",
        "    summarized_articles = []\n",
        "\n",
        "    for i in random_list_first: \n",
        "      response = co.summarize(text=article_contents[i], model='summarize-xlarge', \n",
        "                                length='long', format=\"paragraph\", temperature=0.1,\n",
        "                                additional_command=f\"Write as an equity research analyst that is researching on the current news of {company}\")\n",
        "      summarized_articles.append(response.summary)\n",
        "    \n",
        "    big_string = summarized_articles[0]\n",
        "\n",
        "    for i in range(1,len(big_string)):\n",
        "      big_string += \"\\n\" #doing this to save on memory\n",
        "      big_string += big_string[i]\n",
        "\n",
        "    response = co.summarize(text=big_string, model='summarize-xlarge', \n",
        "                            length='long', format=\"paragraph\", temperature=0.1,\n",
        "                            additional_command=f\"Write as an equity research analyst that is researching on the current news of {company}\")\n",
        "\n",
        "    return sentiment, response.summary\n",
        "\n",
        "statement = sentimental_analysis()\n",
        "\n",
        "print('\\n', 'Overall recent market sentiment: ',\n",
        "      '\\n', statement[0], '\\n', \n",
        "      'Summarization of current market news: ', '\\n',\n",
        "      statement[1])\n",
        "\n",
        "\n",
        "        \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}